{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image as PIL_Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor,MllamaProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, token=\"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\")\n",
    "processor = MllamaProcessor.from_pretrained(model_id, token=\"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\")\n",
    "\n",
    "with open(\"clean.jpeg\", \"rb\") as f:\n",
    "    raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "conversation = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True,tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \",processor.decode(output[0])[len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:34<00:00,  7.00s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "aten::_local_scalar_dense: attempted to run this operator with Meta tensors, but there was no abstract impl or Meta kernel registered. You may have run into this message while using an operator with PT2 compilation APIs (torch.compile/torch.export); in order to use this operator with those APIs you'll need to add an abstract impl.Please see the following doc for next steps: https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-11B-Vision-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_wZfIlBhaIAAveUOVWyuBpnnFIySPhynHfj\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Be cautious with tokens in your code\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m load_quantized_model(model_id, token)\n\u001b[0;32m     32\u001b[0m processor \u001b[38;5;241m=\u001b[39m MllamaProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Load and process the image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [1], line 18\u001b[0m, in \u001b[0;36mload_quantized_model\u001b[1;34m(model_id, token)\u001b[0m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m MllamaForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      9\u001b[0m     model_id, \n\u001b[0;32m     10\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Quantize the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqint8\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantized_model\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:468\u001b[0m, in \u001b[0;36mquantize_dynamic\u001b[1;34m(model, qconfig_spec, dtype, mapping, inplace)\u001b[0m\n\u001b[0;32m    466\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    467\u001b[0m propagate_qconfig_(model, qconfig_spec)\n\u001b[1;32m--> 468\u001b[0m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:553\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[0;32m    552\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[1;32m--> 553\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[0;32m    557\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    590\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m--> 591\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    590\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m--> 591\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "    \u001b[1;31m[... skipping similar frames: _convert at line 591 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    590\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m--> 591\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:593\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    590\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m    591\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[0;32m    592\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[1;32m--> 593\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    596\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:626\u001b[0m, in \u001b[0;36mswap_module\u001b[1;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[0;32m    624\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 626\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\quantized\\dynamic\\modules\\linear.py:111\u001b[0m, in \u001b[0;36mLinear.from_float\u001b[1;34m(cls, mod)\u001b[0m\n\u001b[0;32m    109\u001b[0m weight_observer(mod\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8:\n\u001b[1;32m--> 111\u001b[0m     qweight \u001b[38;5;241m=\u001b[39m \u001b[43m_quantize_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_observer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m    113\u001b[0m     qweight \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\utils.py:56\u001b[0m, in \u001b[0;36m_quantize_weight\u001b[1;34m(float_wt, observer)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantize_weight\u001b[39m(float_wt, observer):\n\u001b[1;32m---> 56\u001b[0m     wt_scale, wt_zp \u001b[38;5;241m=\u001b[39m \u001b[43mobserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_qparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observer\u001b[38;5;241m.\u001b[39mqscheme \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mper_tensor_symmetric, torch\u001b[38;5;241m.\u001b[39mper_tensor_affine]:\n\u001b[0;32m     58\u001b[0m         qweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantize_per_tensor(\n\u001b[0;32m     59\u001b[0m             float_wt,\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;28mfloat\u001b[39m(wt_scale), \u001b[38;5;28mint\u001b[39m(wt_zp), torch\u001b[38;5;241m.\u001b[39mqint8)\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:529\u001b[0m, in \u001b[0;36mMinMaxObserver.calculate_qparams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_qparams\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Calculates the quantization parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_qparams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:328\u001b[0m, in \u001b[0;36mUniformQuantizationObserverBase._calculate_qparams\u001b[1;34m(self, min_val, max_val)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Calculates the quantization parameters, given min and max\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03mvalue tensors. Works for both per tensor and per channel cases\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    zero_points: Zero points tensor of shape (#channels,)\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# Functionally equivalent to 'determine_qparams' in utils.py. Observers must be torchscriptable however and qscheme\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# as far as I can tell is not allowed to passed as a parameter in torchscript functions. This makes refactoring observer\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# to use this utility a massive pain and very gross. For now Im opting just to duplicate as this code\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# seems unlikey to change (last update over 1 year ago) and when torchscript is fully deprecated we can refactor.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# TODO(jakeszwe, jerryzh168)\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_min_max_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m], device\u001b[38;5;241m=\u001b[39mmin_val\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mmin_val\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype)\n\u001b[0;32m    331\u001b[0m quant_min, quant_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_max\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\utils.py:338\u001b[0m, in \u001b[0;36mcheck_min_max_valid\u001b[1;34m(min_val, max_val)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_val\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m max_val\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmin_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m max_val \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    339\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    340\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust run observer before calling calculate_qparams. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    341\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning default values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m         )\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: aten::_local_scalar_dense: attempted to run this operator with Meta tensors, but there was no abstract impl or Meta kernel registered. You may have run into this message while using an operator with PT2 compilation APIs (torch.compile/torch.export); in order to use this operator with those APIs you'll need to add an abstract impl.Please see the following doc for next steps: https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image as PIL_Image\n",
    "from transformers import MllamaForConditionalGeneration, MllamaProcessor\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "def load_quantized_model(model_id, token):\n",
    "    # Load the model\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        model_id, \n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.float32,  # Load in float32 for quantization\n",
    "        token=token,\n",
    "        low_cpu_mem_usage=True,\n",
    "\n",
    "    )\n",
    "    \n",
    "    # Quantize the model\n",
    "    quantized_model = quantize_dynamic(\n",
    "        model, \n",
    "        {torch.nn.Linear}, \n",
    "        dtype=torch.qint8\n",
    "        \n",
    "    )\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "# Set up model and processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "token = \"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\"  # Be cautious with tokens in your code\n",
    "\n",
    "model = load_quantized_model(model_id, token)\n",
    "processor = MllamaProcessor.from_pretrained(model_id, token=token)\n",
    "\n",
    "# Load and process the image\n",
    "with open(\"clean.jpeg\", \"rb\") as f:\n",
    "    raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \", processor.decode(output[0])[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  40%|████      | 2/5 [01:25<02:12, 44.16s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image as PIL_Image\n",
    "from transformers import MllamaForConditionalGeneration, MllamaProcessor\n",
    "\n",
    "# Disable symlinks warning\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "def load_model(model_id, token):\n",
    "    # Load the model\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        token=token\n",
    "    )\n",
    "    \n",
    "    # Move model to GPU if available, otherwise keep on CPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "# Set up model and processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "token = \"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\"  # Remember to use secure methods for token storage\n",
    "\n",
    "model, device = load_model(model_id, token)\n",
    "processor = MllamaProcessor.from_pretrained(model_id, token=token)\n",
    "\n",
    "# Load and process the image\n",
    "with open(\"clean.jpeg\", \"rb\") as f:\n",
    "    raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \", processor.decode(output[0])[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "c:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\utils\\modeling.py:1365: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:26<00:00,  5.34s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "aten::_local_scalar_dense: attempted to run this operator with Meta tensors, but there was no abstract impl or Meta kernel registered. You may have run into this message while using an operator with PT2 compilation APIs (torch.compile/torch.export); in order to use this operator with those APIs you'll need to add an abstract impl.Please see the following doc for next steps: https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-11B-Vision-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_wZfIlBhaIAAveUOVWyuBpnnFIySPhynHfj\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Remember to use secure methods for token storage\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m model \u001b[38;5;241m=\u001b[39m load_quantized_model(model_id, token)\n\u001b[0;32m     39\u001b[0m processor \u001b[38;5;241m=\u001b[39m MllamaProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Load and process the image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [2], line 24\u001b[0m, in \u001b[0;36mload_quantized_model\u001b[1;34m(model_id, token)\u001b[0m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m MllamaForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     15\u001b[0m     model_id, \n\u001b[0;32m     16\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Quantize the model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqint8\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantized_model\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:468\u001b[0m, in \u001b[0;36mquantize_dynamic\u001b[1;34m(model, qconfig_spec, dtype, mapping, inplace)\u001b[0m\n\u001b[0;32m    466\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    467\u001b[0m propagate_qconfig_(model, qconfig_spec)\n\u001b[1;32m--> 468\u001b[0m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:553\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[0;32m    552\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[1;32m--> 553\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[0;32m    557\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    590\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m--> 591\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    590\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m--> 591\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "    \u001b[1;31m[... skipping similar frames: _convert at line 591 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    590\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m--> 591\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:593\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    590\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m    591\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[0;32m    592\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[1;32m--> 593\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    596\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:626\u001b[0m, in \u001b[0;36mswap_module\u001b[1;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[0;32m    624\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 626\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\quantized\\dynamic\\modules\\linear.py:111\u001b[0m, in \u001b[0;36mLinear.from_float\u001b[1;34m(cls, mod)\u001b[0m\n\u001b[0;32m    109\u001b[0m weight_observer(mod\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8:\n\u001b[1;32m--> 111\u001b[0m     qweight \u001b[38;5;241m=\u001b[39m \u001b[43m_quantize_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_observer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m    113\u001b[0m     qweight \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\utils.py:56\u001b[0m, in \u001b[0;36m_quantize_weight\u001b[1;34m(float_wt, observer)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantize_weight\u001b[39m(float_wt, observer):\n\u001b[1;32m---> 56\u001b[0m     wt_scale, wt_zp \u001b[38;5;241m=\u001b[39m \u001b[43mobserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_qparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observer\u001b[38;5;241m.\u001b[39mqscheme \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mper_tensor_symmetric, torch\u001b[38;5;241m.\u001b[39mper_tensor_affine]:\n\u001b[0;32m     58\u001b[0m         qweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantize_per_tensor(\n\u001b[0;32m     59\u001b[0m             float_wt,\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;28mfloat\u001b[39m(wt_scale), \u001b[38;5;28mint\u001b[39m(wt_zp), torch\u001b[38;5;241m.\u001b[39mqint8)\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:529\u001b[0m, in \u001b[0;36mMinMaxObserver.calculate_qparams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_qparams\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Calculates the quantization parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_qparams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:328\u001b[0m, in \u001b[0;36mUniformQuantizationObserverBase._calculate_qparams\u001b[1;34m(self, min_val, max_val)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Calculates the quantization parameters, given min and max\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03mvalue tensors. Works for both per tensor and per channel cases\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    zero_points: Zero points tensor of shape (#channels,)\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# Functionally equivalent to 'determine_qparams' in utils.py. Observers must be torchscriptable however and qscheme\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# as far as I can tell is not allowed to passed as a parameter in torchscript functions. This makes refactoring observer\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# to use this utility a massive pain and very gross. For now Im opting just to duplicate as this code\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# seems unlikey to change (last update over 1 year ago) and when torchscript is fully deprecated we can refactor.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# TODO(jakeszwe, jerryzh168)\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_min_max_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m], device\u001b[38;5;241m=\u001b[39mmin_val\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mmin_val\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype)\n\u001b[0;32m    331\u001b[0m quant_min, quant_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_max\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\utils.py:338\u001b[0m, in \u001b[0;36mcheck_min_max_valid\u001b[1;34m(min_val, max_val)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_val\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m max_val\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmin_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m max_val \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    339\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    340\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust run observer before calling calculate_qparams. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    341\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning default values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m         )\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: aten::_local_scalar_dense: attempted to run this operator with Meta tensors, but there was no abstract impl or Meta kernel registered. You may have run into this message while using an operator with PT2 compilation APIs (torch.compile/torch.export); in order to use this operator with those APIs you'll need to add an abstract impl.Please see the following doc for next steps: https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image as PIL_Image\n",
    "from transformers import MllamaForConditionalGeneration, MllamaProcessor\n",
    "from accelerate import dispatch_model\n",
    "\n",
    "# Disable symlinks warning\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "def load_quantized_model(model_id, token):\n",
    "    # Load the model\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        model_id, \n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.float32,  # Load in float32 for quantization\n",
    "        token=token,\n",
    "        low_cpu_mem_usage=True,\n",
    "\n",
    "    )\n",
    "    \n",
    "    # Quantize the model\n",
    "    quantized_model = quantize_dynamic(\n",
    "        model, \n",
    "        {torch.nn.Linear}, \n",
    "        dtype=torch.qint8\n",
    "        \n",
    "    )\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "\n",
    "# Set up model and processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "token = \"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\"  # Remember to use secure methods for token storage\n",
    "\n",
    "model = load_quantized_model(model_id, token)\n",
    "processor = MllamaProcessor.from_pretrained(model_id, token=token)\n",
    "\n",
    "# Load and process the image\n",
    "with open(\"clean.jpeg\", \"rb\") as f:\n",
    "    raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \", processor.decode(output[0])[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  3.44it/s]\n",
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`checkpoint` should be the path to a file containing a whole state dict, or the index of a sharded checkpoint, or a folder containing a sharded checkpoint or the whole state dict, but got meta-llama/Llama-3.2-11B-Vision-Instruct.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-11B-Vision-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_wZfIlBhaIAAveUOVWyuBpnnFIySPhynHfj\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Remember to use secure methods for token storage\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m model \u001b[38;5;241m=\u001b[39m load_optimized_model(model_id, token)\n\u001b[0;32m     41\u001b[0m processor \u001b[38;5;241m=\u001b[39m MllamaProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Load and process the image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [6], line 20\u001b[0m, in \u001b[0;36mload_optimized_model\u001b[1;34m(model_id, token)\u001b[0m\n\u001b[0;32m     13\u001b[0m     model \u001b[38;5;241m=\u001b[39m MllamaForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     14\u001b[0m         model_id,\n\u001b[0;32m     15\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[0;32m     16\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load the model in 8-bit precision\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint_and_dispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moffload\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# token=token\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Tie weights if possible\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\big_modeling.py:607\u001b[0m, in \u001b[0;36mload_checkpoint_and_dispatch\u001b[1;34m(model, checkpoint, device_map, max_memory, no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict, skip_keys, preload_module_classes, force_hooks, strict)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    606\u001b[0m     offload_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m \u001b[43mload_checkpoint_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\utils\\modeling.py:1647\u001b[0m, in \u001b[0;36mload_checkpoint_in_model\u001b[1;34m(model, checkpoint, device_map, offload_folder, dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb, strict)\u001b[0m\n\u001b[0;32m   1643\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1644\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m containing more than one `.index.json` file, delete the irrelevant ones.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1645\u001b[0m             )\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`checkpoint` should be the path to a file containing a whole state dict, or the index of a sharded \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1649\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint, or a folder containing a sharded checkpoint or the whole state dict, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1650\u001b[0m     )\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     checkpoint_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(index_filename)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: `checkpoint` should be the path to a file containing a whole state dict, or the index of a sharded checkpoint, or a folder containing a sharded checkpoint or the whole state dict, but got meta-llama/Llama-3.2-11B-Vision-Instruct."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image as PIL_Image\n",
    "from transformers import MllamaForConditionalGeneration, MllamaProcessor\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "# Disable symlinks warning\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "def load_optimized_model(model_id, token):\n",
    "    # Initialize an empty model\n",
    "    with init_empty_weights():\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            token=token\n",
    "        )\n",
    "    \n",
    "    # Load the model in 8-bit precision\n",
    "    model = load_checkpoint_and_dispatch(\n",
    "        model, \n",
    "        model_id, \n",
    "        device_map=\"auto\", \n",
    "        offload_folder=\"offload\", \n",
    "        offload_state_dict=True, \n",
    "        dtype=torch.bfloat16,\n",
    "        # token=token\n",
    "    )\n",
    "    \n",
    "    # Tie weights if possible\n",
    "    if hasattr(model, 'tie_weights'):\n",
    "        model.tie_weights()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up model and processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "token = \"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\"  # Remember to use secure methods for token storage\n",
    "\n",
    "model = load_optimized_model(model_id, token)\n",
    "processor = MllamaProcessor.from_pretrained(model_id, token=token)\n",
    "\n",
    "# Load and process the image\n",
    "with open(\"clean.jpeg\", \"rb\") as f:\n",
    "    raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \", processor.decode(output[0])[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'MllamaForConditionalGeneration' has no attribute 'from_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-11B-Vision-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_wZfIlBhaIAAveUOVWyuBpnnFIySPhynHfj\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Remember to use secure methods for token storage\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m load_memory_efficient_model(model_id, token)\n\u001b[0;32m     37\u001b[0m processor \u001b[38;5;241m=\u001b[39m MllamaProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Load and process the image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [7], line 16\u001b[0m, in \u001b[0;36mload_memory_efficient_model\u001b[1;34m(model_id, token)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize an empty model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m init_empty_weights():\n\u001b[1;32m---> 16\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMllamaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m(config)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load the model with CPU offloading\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m load_checkpoint_and_dispatch(\n\u001b[0;32m     20\u001b[0m     model, \n\u001b[0;32m     21\u001b[0m     model_id, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     no_split_module_classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMllamaDecoderLayer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     28\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'MllamaForConditionalGeneration' has no attribute 'from_config'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image as PIL_Image\n",
    "from transformers import MllamaForConditionalGeneration, MllamaProcessor, AutoConfig\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "# Disable symlinks warning\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "def load_memory_efficient_model(model_id, token):\n",
    "    # Load the configuration\n",
    "    config = AutoConfig.from_pretrained(model_id, token=token)\n",
    "    \n",
    "    # Initialize an empty model\n",
    "    with init_empty_weights():\n",
    "        model = MllamaForConditionalGeneration.from_config(config)\n",
    "    \n",
    "    # Load the model with CPU offloading\n",
    "    model = load_checkpoint_and_dispatch(\n",
    "        model, \n",
    "        model_id, \n",
    "        device_map=\"auto\",\n",
    "        offload_folder=\"offload\",\n",
    "        offload_state_dict=True,\n",
    "        dtype=torch.float16,\n",
    "        token=token,\n",
    "        no_split_module_classes=[\"MllamaDecoderLayer\"]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up model and processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "token = \"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\"  # Remember to use secure methods for token storage\n",
    "\n",
    "model = load_memory_efficient_model(model_id, token)\n",
    "processor = MllamaProcessor.from_pretrained(model_id, token=token)\n",
    "\n",
    "# Load and process the image\n",
    "with open(\"clean.jpeg\", \"rb\") as f:\n",
    "    raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \", processor.decode(output[0])[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [01:56<02:55, 58.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-11B-Vision-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_wZfIlBhaIAAveUOVWyuBpnnFIySPhynHfj\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Remember to use secure methods for token storage\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m model \u001b[38;5;241m=\u001b[39m load_memory_efficient_model(model_id, token)\n\u001b[0;32m     29\u001b[0m processor \u001b[38;5;241m=\u001b[39m MllamaProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Load and process the image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [1], line 12\u001b[0m, in \u001b[0;36mload_memory_efficient_model\u001b[1;34m(model_id, token)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_memory_efficient_model\u001b[39m(model_id, token):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Load the model with CPU offloading\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMllamaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Dispatch the model across available devices\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     model \u001b[38;5;241m=\u001b[39m dispatch_model(model, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:4014\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4005\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   4007\u001b[0m     (\n\u001b[0;32m   4008\u001b[0m         model,\n\u001b[0;32m   4009\u001b[0m         missing_keys,\n\u001b[0;32m   4010\u001b[0m         unexpected_keys,\n\u001b[0;32m   4011\u001b[0m         mismatched_keys,\n\u001b[0;32m   4012\u001b[0m         offload_index,\n\u001b[0;32m   4013\u001b[0m         error_msgs,\n\u001b[1;32m-> 4014\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   4018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4021\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4033\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   4034\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:4502\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[0;32m   4498\u001b[0m                 set_module_tensor_to_device(\n\u001b[0;32m   4499\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   4500\u001b[0m                 )\n\u001b[0;32m   4501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4502\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4503\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4505\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4506\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4508\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4509\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4516\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4517\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4518\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[0;32m   4519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4520\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ysyss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:923\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[0;32m    921\u001b[0m             set_module_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 923\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model, and which\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# uses `param.copy_(input_param)` that preserves the contiguity of the parameter in the model.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/blob/db79ceb110f6646523019a59bbd7b838f43d4a86/torch/nn/modules/module.py#L2040C29-L2040C29\u001b[39;00m\n\u001b[0;32m    928\u001b[0m old_param \u001b[38;5;241m=\u001b[39m model\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image as PIL_Image\n",
    "from transformers import MllamaForConditionalGeneration, MllamaProcessor\n",
    "from accelerate import dispatch_model\n",
    "\n",
    "# Disable symlinks warning\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "def load_memory_efficient_model(model_id, token):\n",
    "    # Load the model with CPU offloading\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        token=token\n",
    "    )\n",
    "    \n",
    "    # Dispatch the model across available devices\n",
    "    model = dispatch_model(model, device_map=\"auto\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up model and processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "token = \"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\"  # Remember to use secure methods for token storage\n",
    "\n",
    "model = load_memory_efficient_model(model_id, token)\n",
    "processor = MllamaProcessor.from_pretrained(model_id, token=token)\n",
    "\n",
    "# Load and process the image\n",
    "with open(\"clean.jpeg\", \"rb\") as f:\n",
    "    raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \", processor.decode(output[0])[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أبوجبارة ©\n",
      "امصكم كش 3082165 اودع\n",
      "‎١‏\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "\n",
    "def perform_arabic_ocr(image_path):\n",
    "    # Open the image file\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Configure pytesseract for Arabic\n",
    "    custom_config = r'--oem 3 --psm 6 -l ara'\n",
    "    \n",
    "    # Perform OCR on the image\n",
    "    text = pytesseract.image_to_string(image, config=custom_config)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "image_path = 'clean.jpeg'\n",
    "result = perform_arabic_ocr(image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import NougatProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def perform_arabic_ocr(image_path):\n",
    "    # Load pre-trained model and processor\n",
    "    processor = NougatProcessor.from_pretrained(\"facebook/nougat-base\")\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-base\")\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Prepare the image for the model\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Generate the OCR result\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        max_length=512,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "image_path = 'clean.jpeg'\n",
    "result = perform_arabic_ocr(image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def perform_arabic_ocr(image_path):\n",
    "    # Load pre-trained model and processor\n",
    "    processor = TrOCRProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\",)\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Prepare the image for the model\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Generate the OCR result\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "image_path = 'images.jpeg'\n",
    "result = perform_arabic_ocr(image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
