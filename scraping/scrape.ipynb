{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_news_website(base_url, company_name, max_pages=10):\n",
    "    all_articles = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}page/{page}/\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        articles = soup.find_all('article', class_='post-item')\n",
    "        \n",
    "        for article in articles:\n",
    "            title = article.find('h2', class_='post-title').text.strip()\n",
    "            \n",
    "            if company_name.lower() in title.lower():\n",
    "                link = article.find('a', class_='post-title-link')['href']\n",
    "                date = article.find('span', class_='date').text.strip()\n",
    "                \n",
    "                # Scrape full article content\n",
    "                article_response = requests.get(link)\n",
    "                article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "                content = article_soup.find('div', class_='post-content').text.strip()\n",
    "                \n",
    "                all_articles.append({\n",
    "                    'title': title,\n",
    "                    'date': date,\n",
    "                    'content': content,\n",
    "                    'url': link\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(all_articles)\n",
    "\n",
    "# Usage\n",
    "base_url = \"https://maaal.com/news-cat/%d8%a7%d9%84%d8%a3%d8%ae%d8%a8%d8%a7%d8%b1-%d8%a7%d9%84%d8%a7%d9%82%d8%aa%d8%b5%d8%a7%d8%af%d9%8a%d8%a9/\"\n",
    "company_name = \"GO\"\n",
    "df = scrape_news_website(base_url, company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1\n",
      "t2\n",
      "Finished scraping page 1\n",
      "Total articles found: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove extra whitespace and newlines\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def scrape_news_website(base_url, company_name, max_pages=1):\n",
    "    all_articles = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}page/{page}/\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        print(\"t1\")\n",
    "        \n",
    "        articles = soup.find_all('article', class_='jeg_post')\n",
    "        print(\"t2 articles all :\", articles)\n",
    "        for article in articles:\n",
    "            print(\"t3\")\n",
    "            title_element = article.find('h3', class_='jeg_post_title')\n",
    "            if title_element:\n",
    "                title = clean_text(title_element.text)\n",
    "                print(\"searching for company: \", title)\n",
    "                print(\"in website data:\", title_element)\n",
    "                \n",
    "                if company_name.lower() in title.lower():\n",
    "                    link = title_element.find('a')['href']\n",
    "                    \n",
    "                    # Scrape full article content\n",
    "                    article_response = requests.get(link)\n",
    "                    article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "                    \n",
    "                    # Extract date\n",
    "                    date_element = article_soup.find('div', class_='jeg_meta_date')\n",
    "                    date = clean_text(date_element.text) if date_element else \"Unknown Date\"\n",
    "                    \n",
    "                    # Extract content\n",
    "                    content_element = article_soup.find('div', class_='content-inner')\n",
    "                    content = clean_text(content_element.text) if content_element else \"No content found\"\n",
    "                    \n",
    "                    all_articles.append({\n",
    "                        'title': title,\n",
    "                        'date': date,\n",
    "                        'content': content,\n",
    "                        'url': link\n",
    "                    })\n",
    "                    \n",
    "                    # Add a small delay to be respectful to the website\n",
    "                    time.sleep(1)\n",
    "        \n",
    "        print(f\"Finished scraping page {page}\")\n",
    "        \n",
    "        # Add a small delay between pages\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return pd.DataFrame(all_articles)\n",
    "\n",
    "# Usage\n",
    "base_url = \"https://maaal.com/news-cat/%d8%a7%d9%84%d8%a3%d8%ae%d8%a8%d8%a7%d8%b1-%d8%a7%d9%84%d8%a7%d9%82%d8%aa%d8%b5%d8%a7%d8%af%d9%8a%d8%a9/\"\n",
    "company_name = \"توقيع مذكرة تفاهم بين شركة البيك للأنظمة الغذائية و شركة GO الباكستانية لدراسة فرص تواجد البيك في باكستان\"  # Replace with the actual company name\n",
    "df = scrape_news_website(base_url, company_name)\n",
    "\n",
    "print(f\"Total articles found: {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام\n"
     ]
    }
   ],
   "source": [
    "s = \"سلام\"\n",
    "print(s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
